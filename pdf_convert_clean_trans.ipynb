{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775add4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# åŸå§‹ PDF æ–‡ä»¶è·¯å¾„\n",
    "pdf_path = r\"D:\\workspace\\pdf_convert_clean_trans\\input\\ISO 26262-1-2018.pdf\"\n",
    "\n",
    "# è·å–æ–‡ä»¶åå¹¶æ›¿æ¢å­—ç¬¦\n",
    "filename = os.path.basename(pdf_path)\n",
    "new_filename = filename.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "new_pdf_path = os.path.join(os.getcwd(), \".tmp\", new_filename)\n",
    "\n",
    "# åˆ›å»º .tmp æ–‡ä»¶å¤¹\n",
    "tmp_dir = os.path.dirname(new_pdf_path)\n",
    "\n",
    "# å°†åŸå§‹ PDF æ–‡ä»¶å¤åˆ¶åˆ° .tmp æ–‡ä»¶å¤¹\n",
    "shutil.copy(pdf_path, new_pdf_path)\n",
    "\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "# æ„é€ è¾“å‡ºç›®å½•\n",
    "output_dir = os.path.join(tmp_dir, \"origin\")\n",
    "\n",
    "# åˆ›å»º origin å­æ–‡ä»¶å¤¹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# è°ƒç”¨ mineru å‘½ä»¤è¡Œå·¥å…·è¿›è¡Œ PDF è½¬æ¢\n",
    "result = subprocess.run(\n",
    "    [\"mineru\", \"-p\", new_pdf_path, \"-o\", output_dir],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# æ£€æŸ¥å‘½ä»¤æ‰§è¡Œç»“æœ\n",
    "if result.returncode == 0:\n",
    "    print(\"å‘½ä»¤æ‰§è¡ŒæˆåŠŸ\")\n",
    "    print(\"è¾“å‡ºï¼š\", result.stdout)\n",
    "else:\n",
    "    print(\"å‘½ä»¤æ‰§è¡Œå¤±è´¥\")\n",
    "    print(\"é”™è¯¯ï¼š\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a1c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰å†…å®¹æå–å¹¶é‡å‘½åå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = \"./.tmp/origin\"\n",
    "EXTRACTED_DIR = \"./.tmp/extracted_origin\"\n",
    "\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"ç»Ÿä¸€æ›¿æ¢åç§°ä¸­çš„ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿\"\"\"\n",
    "    return name.replace(\" \", \"_\")\n",
    "\n",
    "def extract_auto_contents(input_dir: str, output_dir: str):\n",
    "    input_path = Path(input_dir)\n",
    "\n",
    "    for sub_dir in input_path.iterdir():\n",
    "        if not sub_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        auto_path = sub_dir / \"auto\"\n",
    "        if not auto_path.exists():\n",
    "            continue\n",
    "\n",
    "        # æ›¿æ¢å­æ–‡ä»¶å¤¹åä¸­çš„ç©ºæ ¼\n",
    "        sanitized_subdir_name = sanitize_name(sub_dir.name)\n",
    "        target_subdir = os.path.join(output_dir, sanitized_subdir_name)\n",
    "        os.makedirs(target_subdir, exist_ok=True)\n",
    "\n",
    "        # æ‹·è´ auto ä¸‹çš„ Markdown æ–‡ä»¶ï¼ˆ*.mdï¼‰\n",
    "        for md_file in auto_path.glob(\"*.md\"):\n",
    "            if md_file.is_file():\n",
    "                sanitized_filename = sanitize_name(md_file.name)\n",
    "                shutil.copy2(md_file, os.path.join(target_subdir, sanitized_filename))\n",
    "\n",
    "        # æ‹·è´ auto/images ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶\n",
    "        image_dir = auto_path / \"images\"\n",
    "        if image_dir.exists() and image_dir.is_dir():\n",
    "            target_image_dir = os.path.join(target_subdir, \"images\")\n",
    "            os.makedirs(target_image_dir, exist_ok=True)\n",
    "\n",
    "            for img_file in image_dir.rglob(\"*.*\"):\n",
    "                if img_file.is_file():\n",
    "                    sanitized_img_name = sanitize_name(img_file.name)\n",
    "                    shutil.copy2(img_file, os.path.join(target_image_dir, sanitized_img_name))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_auto_contents(INPUT_DIR, EXTRACTED_DIR)\n",
    "    print(\"âœ… æ‰€æœ‰å†…å®¹æå–å¹¶é‡å‘½åå®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957aac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from llama_index.core.utils import count_tokens\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "from llama_index.embeddings.openai_like import OpenAILikeEmbedding\n",
    "\n",
    "# ========== é…ç½®éƒ¨åˆ† ==========\n",
    "EXTRACTED_DIR = \"./.tmp/extracted_origin\"\n",
    "CLEANED_DIR = \"./.tmp/cleaned\"\n",
    "TRANS_DIR = \"./.tmp/translated\"\n",
    "DEBUG = True\n",
    "\n",
    "DEBUG_CLEAN_DIR = \"./.tmp/debug_clean_chunk\"\n",
    "DEBUG_TRANS_DIR = \"./.tmp/debug_trans_chunk\"\n",
    "\n",
    "MAX_TOKENS_PER_CHUNK = 6000\n",
    "\n",
    "# æ—¥å¿—é…ç½®\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"errors.log\", encoding=\"utf-8\")]\n",
    ")\n",
    "\n",
    "# LLM é…ç½®\n",
    "Settings.llm = DeepSeek(model=\"deepseek-chat\", api_key=\"sk-eac019be79f14f948591d963d8c17656\")\n",
    "Settings.embed_model = OpenAILikeEmbedding(\n",
    "    model_name=\"text-embedding-v4\",\n",
    "    api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=\"sk-e9aeb7dc9a3e4bf784411b295ddfa402\",\n",
    "    embed_batch_size=10,\n",
    ")\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 64\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "clean_prompt = PromptTemplate(r\"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Markdown æ ¼å¼ä¿®å¤ä¸“å®¶ã€‚ä½ å°†æ¥æ”¶ç”± OCR æˆ– PDF è½¬æ¢å·¥å…·ï¼ˆå¦‚ Mineruï¼‰ç”Ÿæˆçš„ Markdown æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶æ ¼å¼å­˜åœ¨ä¸¥é‡é”™è¯¯ï¼Œè¯·ä½ å¯¹å…¶è¿›è¡Œä»¥ä¸‹ä¿®å¤ï¼Œä¿æŒè¯­ä¹‰å’Œç»“æ„ä¸å˜ï¼Œä»…ä¿®å¤æ ¼å¼é—®é¢˜ã€‚\n",
    "\n",
    "è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š\n",
    "\n",
    "1. **ä¿®å¤æ ‡é¢˜æ ¼å¼ï¼š**\n",
    "   - æ‰€æœ‰å½¢å¦‚ `# N` çš„æ ‡é¢˜ä¸ºä¸€çº§æ ‡é¢˜ï¼ˆä¾‹å¦‚ `# 1`ã€`# 2`ï¼‰ï¼›\n",
    "   - æ‰€æœ‰å½¢å¦‚ `N.N` çš„åº”ä¸ºäºŒçº§æ ‡é¢˜ `## N.N`ï¼›\n",
    "   - æ‰€æœ‰å½¢å¦‚ `N.N.N` çš„ä¸ºä¸‰çº§æ ‡é¢˜ `### N.N.N`ï¼›\n",
    "   - å¦‚æœæ ‡é¢˜å‰å­˜åœ¨é”™è¯¯çš„ `#` ä¸ªæ•°ï¼ˆå¦‚ `### 3.3` åº”æ˜¯ `## 3.3`ï¼‰ï¼Œè¯·çº æ­£ã€‚\n",
    "\n",
    "2. **ä¿®å¤åˆ—è¡¨æ ¼å¼ï¼š**\n",
    "   - æ‰€æœ‰ä»¥ `â€”` æˆ– `â€“`ã€é•¿ç ´æŠ˜å·æˆ–é”™è¯¯ç¬¦å·å¼€å¤´çš„æ¡ç›®ï¼Œåº”ç»Ÿä¸€ä¸º `-`ï¼ˆæ ‡å‡† Markdown æ— åºåˆ—è¡¨ï¼‰ï¼›\n",
    "   - è‹¥åµŒå¥—åˆ—è¡¨å­˜åœ¨ç¼©è¿›é”™è¯¯ï¼ŒæŒ‰ç…§ 2 ä¸ªç©ºæ ¼ä½œä¸ºæ¯ä¸€çº§ç¼©è¿›è¿›è¡Œè§„èŒƒï¼›\n",
    "   - è‹¥å¤šçº§åˆ—è¡¨ä¸­å­˜åœ¨è‡ªç„¶è¯­è¨€æ®µè½æè¿°ï¼Œè¯·ç¡®ä¿å­é¡¹ç¼©è¿›æ­£ç¡®ï¼Œå¹¶åœ¨æ¯é¡¹ååŠ æ¢è¡Œã€‚\n",
    "\n",
    "3. **ä¿®å¤ NOTE æ ‡æ³¨ï¼š**\n",
    "   - å°†ç±»ä¼¼â€œNote 1 to entry: â€¦â€ è½¬æ¢ä¸ºä»¥ä¸‹æ ‡å‡†æ ¼å¼ï¼š\n",
    "     ```markdown\n",
    "     > [!NOTE]\n",
    "     > Note 1 to entry: ...\n",
    "     ```\n",
    "   - åŒä¸€æ®µä¸­å¤šä¸ª Noteï¼Œä¾æ¬¡ç¼–å·ï¼Œä¿æŒç¼©è¿›ä¸€è‡´ã€‚\n",
    "\n",
    "4. **ä¿®å¤æ•°å­¦å…¬å¼æ±¡æŸ“ï¼š**\n",
    "   - è‹¥å‡ºç° LaTeX æ•°å­¦è¯­æ³•æœªæ­£ç¡®æ¸²æŸ“ï¼Œä¾‹å¦‚ï¼š\n",
    "     ```markdown\n",
    "     $\" \\mathrm { m } { \\cdot } \\mathrm { n } ^ { \\prime \\prime }$ \n",
    "     ```\n",
    "     åº”å°½é‡è½¬æ¢ä¸ºæ­£å¸¸æ–‡æœ¬ï¼ˆå¦‚ `mÂ·nâ€³`ï¼‰ï¼Œæˆ–ä¿ç•™ä¸ºè¡Œå†…å…¬å¼ `\\( m \\cdot n^{\\prime\\prime} \\)`ã€‚\n",
    "\n",
    "5. **ä¿®å¤ä¹±ç å’Œä¸å¿…è¦ç¬¦å·ï¼š**\n",
    "   - åˆ é™¤æ— æ•ˆè½¬ä¹‰ç¬¦ï¼ˆå¦‚ `\\`ã€`\\``, `\\`, `,` ç­‰æˆå¯¹æ··ä¹±å‡ºç°çš„æ ‡ç‚¹ï¼‰ï¼›\n",
    "   - ä¿®å¤ markdown ä¸­å›¾åƒè¯­æ³•é”™è¯¯ï¼ˆå¦‚ `![]()`ï¼‰ä¸­è·¯å¾„ä¸¢å¤±æˆ–æ³¨é‡Šæ··ä¹±çš„é—®é¢˜ï¼›\n",
    "   - åˆ é™¤é‡å¤ç©ºæ ¼å’Œç©ºè¡Œï¼Œç¡®ä¿æ®µè½ä¹‹é—´æœ€å¤šä¿ç•™ä¸€ä¸ªç©ºè¡Œã€‚\n",
    "\n",
    "6. **ä¿æŒå†…å®¹åŸå§‹è¯­ä¹‰ä¸å˜**ï¼Œä»…åšæ ¼å¼æ¸…æ´—ã€‚\n",
    "\n",
    "è¾“å‡ºæ ¼å¼ä¸º**ä¿®å¤åçš„å®Œæ•´ Markdown æ–‡ä»¶**ã€‚ä¸è¦è¿›è¡Œé¢å¤–è§£é‡Šæˆ–æ³¨é‡Šã€‚\n",
    "\n",
    "è¯·æ¸…æ´—ä»¥ä¸‹ Markdown æ–‡æœ¬ï¼š\n",
    "\n",
    "{context_str}\n",
    "\"\"\")\n",
    "\n",
    "translation_prompt = PromptTemplate(r\"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Markdown æ–‡æ¡£ç¿»è¯‘æœºå™¨äººï¼Œè´Ÿè´£å°†è‹±æ–‡ Markdown æ–‡æ¡£ç¿»è¯‘ä¸ºè‡ªç„¶ã€å‡†ç¡®çš„ä¸­æ–‡ã€‚ä½ æ”¶åˆ°çš„ Markdown æ–‡æ¡£å¯èƒ½æ˜¯ç»è¿‡åˆ‡åˆ†çš„ç‰‡æ®µï¼Œç»“æ„å¯èƒ½ä¸å®Œæ•´ï¼Œå­˜åœ¨ç¼ºå°‘ä¸€çº§æ ‡é¢˜ã€ä»ä¸­é—´æ®µè½å¼€å§‹æˆ–æ ‡é¢˜å±‚çº§ä¸è§„èŒƒç­‰æƒ…å†µã€‚è¯·ä¸¥æ ¼ä¿ç•™åŸæ–‡ Markdown æ ¼å¼ï¼Œä¸å¾—æ›´æ”¹æ ‡é¢˜å±‚çº§æˆ–ç ´åä»»ä½•ç»“æ„æ€§è¯­æ³•ã€‚\n",
    "\n",
    "è¯·éµå¾ªä»¥ä¸‹ç¿»è¯‘è§„åˆ™ï¼š\n",
    "\n",
    "1. **ä¸¥æ ¼ä¿ç•™æ‰€æœ‰ Markdown ç»“æ„å’Œè¯­æ³•**ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ ‡é¢˜ï¼ˆå¦‚ `#` `##` `###` `####` ç­‰ï¼‰ã€åˆ—è¡¨ï¼ˆæœ‰åº/æ— åºï¼‰ã€é“¾æ¥ã€å›¾ç‰‡ã€ä»£ç å—ã€è¡¨æ ¼ã€å¼•ç”¨ã€åˆ†éš”çº¿ç­‰ï¼Œä¸å¾—å¢åˆ æˆ–è°ƒæ•´å±‚çº§ï¼›\n",
    "2. **æ‰€æœ‰æ ‡é¢˜ç¬¦å·å¿…é¡»ä¸¥æ ¼ä¿ç•™ï¼Œæ ‡é¢˜ç­‰çº§ä¸å¯æ›´æ”¹**ï¼›\n",
    "3. **æ‰€æœ‰ä»£ç å—ï¼ˆ```ï¼‰å’Œè¡Œå†…ä»£ç ï¼ˆ`code`ï¼‰å¿…é¡»å®Œæ•´ä¿ç•™ï¼Œç¦æ­¢ç¿»è¯‘æˆ–æ›´åŠ¨ä»»ä½•å­—ç¬¦**ï¼›\n",
    "4. **å›¾ç‰‡å’Œé“¾æ¥ä¸­çš„ URLã€æ–‡ä»¶åã€è·¯å¾„ç­‰ä¿æŒåŸæ ·ï¼Œä¸å¾—ç¿»è¯‘æˆ–ä¿®æ”¹**ï¼›\n",
    "5. æ­£æ–‡å†…å®¹åº”ç¿»è¯‘ä¸ºä¸“ä¸šã€é€šé¡ºã€è‡ªç„¶çš„ä¸­æ–‡ï¼›\n",
    "6. **ä»…è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡ Markdown å†…å®¹**ï¼›\n",
    "7. å³ä½¿è¾“å…¥è¢«åˆ‡åˆ†ä¸ºä¸å®Œæ•´æ®µè½ï¼Œä¹Ÿ**ä¸å¾—ç ´ååŸæœ‰ Markdown çš„ç»“æ„**ï¼›\n",
    "\n",
    "è¯·ç¿»è¯‘ä»¥ä¸‹ Markdown æ–‡æœ¬ï¼š\n",
    "\n",
    "{context_str}\n",
    "\"\"\")\n",
    "\n",
    "# ========== å·¥å…·å‡½æ•° ==========\n",
    "def extract_code_blocks(text: str) -> Tuple[str, List[str]]:\n",
    "    code_blocks = []\n",
    "    def replacer(match):\n",
    "        code_blocks.append(match.group(0))\n",
    "        return f\"__CODE_BLOCK_{len(code_blocks) - 1}__\"\n",
    "    safe_text = re.sub(r\"```.*?\\n.*?```\", replacer, text, flags=re.DOTALL)\n",
    "    return safe_text, code_blocks\n",
    "\n",
    "def restore_code_blocks(text: str, code_blocks: List[str]) -> str:\n",
    "    for i, block in enumerate(code_blocks):\n",
    "        text = text.replace(f\"__CODE_BLOCK_{i}__\", block)\n",
    "    return text\n",
    "\n",
    "def split_markdown_by_heading(text: str, max_chars_per_chunk: int = 3000) -> List[str]:\n",
    "    safe_text, code_blocks = extract_code_blocks(text)\n",
    "    headings = list(re.finditer(r'^(#{1,6}\\s+.*)', safe_text, re.MULTILINE))\n",
    "    \n",
    "    sections = []\n",
    "    for i, match in enumerate(headings):\n",
    "        start = match.start()\n",
    "        end = headings[i + 1].start() if i + 1 < len(headings) else len(safe_text)\n",
    "        sections.append(safe_text[start:end].strip())\n",
    "\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for section in sections:\n",
    "        if len(current_chunk) + len(section) < max_chars_per_chunk:\n",
    "            current_chunk += section + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = section + \"\\n\\n\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return [restore_code_blocks(chunk, code_blocks) for chunk in chunks]\n",
    "\n",
    "def run_cleaning(chunks: List[str], prompt_template, llm) -> List[str]:\n",
    "    results = []\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"ğŸ§¹ Cleaning Chunks\", leave=False)):\n",
    "        try:\n",
    "            prompt = prompt_template.format(context_str=chunk)\n",
    "            response = llm.complete(prompt)\n",
    "            results.append(response.text.strip())\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[Chunk {i+1}] æ¸…æ´—å¤±è´¥: {e}\")\n",
    "            results.append(f\"<!-- æ¸…æ´—å¤±è´¥ï¼š{e} -->\")\n",
    "    return results\n",
    "\n",
    "def write_debug_chunks(original_chunks, processed_chunks, rel_path: str):\n",
    "    sep = \"\\n\" + \"=\" * 100 + \"\\n\"\n",
    "    rel_md_path = Path(rel_path).with_suffix(\".md\")\n",
    "\n",
    "    # åŸå§‹åˆ‡ç‰‡\n",
    "    input_path = Path(DEBUG_CLEAN_DIR) / (\"input_\"+rel_md_path)\n",
    "    input_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    input_path.write_text(sep.join(original_chunks), encoding=\"utf-8\")\n",
    "\n",
    "    # å¤„ç†ååˆ‡ç‰‡\n",
    "    output_path = Path(DEBUG_CLEAN_DIR) / (\"processed_\"+rel_md_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_path.write_text(sep.join(processed_chunks), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def clean_md(input_path: str, output_path: str, debug: bool = False,\n",
    "             prompt_template=None, llm=None, input_dir=None, debug_input_dir=None, \n",
    "             debug_output_dir=None, max_tokens=6000):\n",
    "    text = Path(input_path).read_text(encoding=\"utf-8\")\n",
    "    chunks = split_markdown_by_heading(text, max_chars_per_chunk=max_tokens)\n",
    "    results = run_cleaning(chunks, prompt_template, llm)\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(output_path).write_text(\"\\n\\n\".join(results), encoding=\"utf-8\")\n",
    "\n",
    "    if debug:\n",
    "        rel_path = os.path.relpath(input_path, input_dir)\n",
    "        write_debug_chunks(chunks, results, rel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d640842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def idisplay_markdown(markdown_text: str):\n",
    "    \"\"\"åœ¨ Jupyter ä¸­æ¸²æŸ“æ˜¾ç¤º Markdown å†…å®¹\"\"\"\n",
    "    display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f2649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55c836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15edf9f1bf74298a6568bf0c9c09974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ§¹ Cleaning Files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d26cc048ce4d4f9d473ac4a2a12b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ§¹ Cleaning Chunks:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = list(Path(EXTRACTED_DIR).rglob(\"*.*\"))\n",
    "\n",
    "for file_path in tqdm(files, desc=\"ğŸ§¹ Cleaning Files\"):\n",
    "    rel_path = os.path.relpath(file_path, EXTRACTED_DIR)\n",
    "    output_path = os.path.join(CLEANED_DIR, rel_path)\n",
    "\n",
    "    try:\n",
    "        if file_path.suffix.lower() in [\".md\", \".mdx\"]:\n",
    "            clean_md(\n",
    "                input_path=str(file_path),\n",
    "                output_path=output_path,\n",
    "                debug=DEBUG,\n",
    "                prompt_template=clean_prompt,\n",
    "                llm=Settings.llm,\n",
    "                input_dir=EXTRACTED_DIR,\n",
    "                debug_input_dir=DEBUG_CLEAN_DIR,\n",
    "                debug_output_dir=DEBUG_CLEAN_DIR,\n",
    "                max_tokens=MAX_TOKENS_PER_CHUNK\n",
    "            )\n",
    "        else:\n",
    "            shutil.copyfile(file_path, output_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[{rel_path}] æ–‡ä»¶æ¸…æ´—å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95008090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a766c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
